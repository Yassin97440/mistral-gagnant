# ğŸ§  Mistral-Gagnant

## Ã€ propos
Un projet d'assistant personnel IA utilisant **Mistral AI** avec implÃ©mentation de techniques RAG (Retrieval-Augmented Generation) et fine-tuning.

## ğŸ“š Structure du projet
- **front/** : Application frontend basÃ©e sur Nuxt.js et Vuetify
- **RAG_TS/** : ImplÃ©mentation du systÃ¨me RAG en TypeScript avec LangChain
- **python_experimentation/** : Notebooks d'exploration et visualisation des embeddings

## ğŸš€ Guide de dÃ©marrage

### PrÃ©requis
- Node.js (version rÃ©cente)
- NPM/Yarn
- ClÃ©s API requises:
  - Mistral AI
  - Hugging Face (pour les embeddings)
  - Notion (optionnel, pour l'intÃ©gration)

### Configuration des variables d'environnement
- Pour le frontend: Copier `.env.exemple` vers `.env` dans le dossier `front/`
- Pour le RAG: Copier `varEnv.exemple.ts` vers `varEnv.ts` dans le dossier `RAG_TS/`

### Frontend (Nuxt.js)
```bash
cd front
npm install
npm run dev
```

### Module RAG
```bash
cd RAG_TS
npm install
npm start
docker pull chromadb/chroma:0.6.3 
```

## ğŸ”§ Technologies utilisÃ©es
- **LLM**: Mistral AI API
- **RAG**: LangChain.js, ChromaDB
- **Embeddings**: Sentence Transformers via Hugging Face
- **Frontend**: Nuxt.js, Vuetify, TailwindCSS
- **IntÃ©grations**: API Notion

## ğŸ“Š ExpÃ©rimentations Python
Le dossier `python_experimentation/` contient plusieurs notebooks:
- `embeddings_visualizer.ipynb`: Visualisation des embeddings
- `run-local-llm.ipynb`: ExpÃ©rimentations avec LLM local
- `auto-rag.ipynb` et `poc-rag.ipynb`: Exploration des techniques RAG

## ğŸ“ Licence
Apache 2.0